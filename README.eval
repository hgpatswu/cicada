MT evaluations especially for MERT purpose

BLEU: IBM Bleu (TODO: implement NIST-bleu, average-bleu?)
WER: word error rate
PER: position independent word error rate
TER: translation error rate
SB: skip bigram (by default, skip size is clipped to 4)
WLCS: (weighted) longest common subsequence (by defaut, weight is one, meaning no weight, use alpha=2 for weight?)
SK: string kernel (by default, decay factor = 0.8, spectrum p = 4)

Implemented bootstrap resampling[1] and sign test[2] in "cicada_eval"

cicada_eval [options]

configuration options:
  --tstset arg                 test set file(s)
  --tstset2 arg                test set file(s)   (required for sign test)
  --refset arg                 reference set file(s)
  --output arg (=-)            output file
  --scorer arg (=bleu:order=4) error metric
  --scorer-list                list of error metric
  --signtest                   sign test
  --bootstrap                  bootstrap resampling
  --samples arg                # of samples

reference/test set format is as follows:

segment-id |||  sentence (||| .... some information, such as features etc. ...)

Thus, you can directly feed k-best output from cicada(or cicada_mpi).

Also, you can compute oracle-BLEU from hypergraph(s) by cicada_oracle{,_mpi}

Tips: cicada_oracle{,_mpi} assumes "forests" as a input. If you want to compute
 oracle score for k-bests, you can convert the k-bests as a forest, by first
converting the sentence set as a lattice using cicada_unite_lattice, then, use cicada
to intersect with a straight glue-rule.

encode/decode API for scorer:

std::string score.encode();
score_ptr score::decode(std::string);

Evaluator score format:
{"eval":"bleu", "reference":["base64" (length), "base64" (1gram), "base64", "base64", "base64"], "hypothesis":[...]}
{"eval":"ter", "insertion":"base64", "deletion":"", ...}
{"eval":"wer", "insertion":"base64", "deletion":"", ...}
{"eval":"per", "insertion":"base64", "deletion":"", ...}
{"eval":"sk", "reference":["base64", ""], "hypothesis":["base64", ...]}
{"eval":"sb", "reference":["base64", ""], "hypothesis":["base64", ...]}
{"eval":"wlcs", "reference":["base64", ""], "hypothesis":["base64", ...]}
{"eval":"combined", [{"eval":"bleu", ...}, {"eval":"ter", ...}], ["base64", "base64",.... list of weights]}
and doubles are encoded as base64 (string!)

[1]
@inproceedings{koehn:2004:EMNLP,
  author    = {Koehn, Philipp},
  title     = {Statistical Significance Tests for Machine Translation Evaluation },
  booktitle = {Proceedings of EMNLP 2004},
  editor = {Dekang Lin and Dekai Wu},
  year      = 2004,
  month     = {July},
  address   = {Barcelona, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {388--395}
}

[2]
@inproceedings{1219906,
 author = {Collins, Michael and Koehn, Philipp and Ku\v{c}erov\'{a}, Ivona},
 title = {Clause restructuring for statistical machine translation},
 booktitle = {ACL '05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
 year = {2005},
 pages = {531--540},
 location = {Ann Arbor, Michigan},
 doi = {http://dx.doi.org/10.3115/1219840.1219906},
 publisher = {Association for Computational Linguistics},
 address = {Morristown, NJ, USA},
 }


