Phrase/Rule/Tree-fragment extraction

We support various phrase/rule/tree extraction. Basically, we will run in two steps:
   Collect counts by mapping input sentences/parse trees/alignment etc.
   Map/Reduce to collect target-side counts
   Map/Reduce to collect source/target counts and produce various counts

   They can operate similarly with underlying extraction specific map/reduce framework.

You can try "cicada-extract.py" which runs like phrase-extract in Moses, but starting from step 4 (and 6).
For details, see the last part of this document.
Internally, cicada-extract.py calls following binaries:

cicada_extract_phrase{,_mpi}
	Extract phrase
	Output scores:
	       lhs ||| rhs ||| count(lhs, rhs) \
	       	   count(prev, mono, lhs, rhs) \
		   count(prev, swap, lhs, rhs) \
		   count(next, mono, lhs, rhs) \
		   count(next, swap, lhs, rhs)

cicada_extract_rule{,_mpi}
	Extract synchronous-CFG + syntax augmentation (aka SAMT) when extracted with "span" data.
	You can generate span by "cicada_filter_penntreebank"
	Output scores:
	       root lhs ||| root rhs ||| count(lhs, rhs)

cicada_extract_ghkm{,_mpi}
	Extract GHKM rules, tree-to-string or string-to-tree rules
	Output scores:
	       lhs-xRS ||| rhs-xRS ||| count(lhs, rhs)
	TODO: How to support dumping in string-to-tree?
	      add --inverse option for inversed alignment,
	      add --swap option for swapping source and target?

cicada_extract_tree{,_mpi}
	Extract GHKM rules, tree-to-tree rules
	Output scores:
	       lhs-xRS ||| rhs-xRS ||| count(lhs, rhs)

After counts collection, you can summarize them by cicada_extract_counts{,_mpi}
Which will output:

lhs ||| rhs ||| counts(lhs, rhs) ||| counts(lhs) ||| counts(rhs) ||| observed(lhs) observed(rhs) \
    	||| lex(lhs | rhs) lex(rhs | lhs)

In addtion, we will dump root-source.gz and root-target.gz. root-source.gz contains:

root(lhs) ||| counts(root(lhs)) ||| observed(lhs,rhs) observed(lhs)

while root-target.gz looks like:

root(rhs) ||| counts(root(rhs)) ||| observed(hs,rhs) observed(rhs)

You can easily transform the counts into probabilities by maximum likelihood estimates, or use observed counts
to perform Dirichlet prior smoothing (default).
Following filers are implemented:

   cicada_filter_extract:
	extract only nbet of target variation for each source side,
   	measured by its joint frequency of lhs and rhs.

   cicada_filter_extract_phrase:
	Dump in moses or cicada format. Also, you can dump
	lexicalied reordering table.
   				 
   cicada_filter_extract_scfg:
	Dump in cicada format for synchronous-CFG. You can also 
	add features for lhs given root(lhs) and rhs given root(rhs)

   cicada_filter_extract_ghkm:
	Dump in cicada format for tree-to-string, string-to-tree, tree-to-tree.

Usage: cicada-extract.py [options]

Options:

Directory setup:
  --root-dir=DIRECTORY  root directory for outputs
  --corpus-dir=PREFIX   corpus directory (default: ${root_dir}/corpus)
  --giza-f2e=DIRECTORY  giza directory for P(f|e) (default: ${root_dir}/giza.${f}-${e})
  --giza-e2f=DIRECTORY  giza directory for P(e|f) (default: ${root_dir}/giza.${e}-${f})
  --model-dir=DIRECTORY
                        model directory (default: ${root_dir}/model)
  --alignment-dir=DIRECTORY
                        alignment directory (default: ${model_dir})
  --lexical-dir=DIRECTORY
                        lexical transltion table directory (default: ${model_dir})

Corpus suffix options:
  --f=SUFFIX            source (or 'French')  language suffix for training corpus
  --e=SUFFIX            target (or 'English') language suffix for training corpus
  --sf=SUFFIX           source (or 'French')  span suffix for training corpus
  --se=SUFFIX           target (or 'English') span suffix for training corpus
  --ff=SUFFIX           source (or 'French')  forest suffix for training corpus
  --fe=SUFFIX           target (or 'English') forest suffix for training corpus

Corpus: (we will differentiate various data by the suffix above)
  --corpus=CORPUS       bilingual trainging corpus

Alignment data: (we will use the alignment data located at ${model_dir}/aligned.${alignment})
  --alignment=ALIGNMENT
                        alignment methods (default: grow-diag-final-and)

Steps:
  --first-step=STEP     first step (default: 5)
  --last-step=STEP      last step  (default: 6)

Dirichlet prior parameter for Lexical model learning:
  --lexicon-prior=PRIOR
                        lexicon model prior (default: 0.1)

Extraction algorithms:
  --phrase              extract phrase
  --scfg                extract SCFG
  --ghkm                extract GHKM (tree-to-string)
  --tree                extract tree-to-tree

Default non-terminal for GHKM/Tree-to-tree extraction
  --non-terminal=NON_TERMINAL
                        default non-terminal for GHKM rule (default: [x])

Parameters for extraction:
  --max-span=LENGTH     maximum span size (default: 15)
  --min-hole=LENGTH     minimum hole size (default: 1)
  --max-length=LENGTH   maximum terminal length (default: 7)
  --max-fertility=FERTILITY
                        maximum terminal fertility (default: 4)
  --max-nodes=NODES     maximum rule nodes (default: 15)
  --max-height=HEIGHT   maximum rule height (default: 4)
  --exhaustive          exhaustive extraction in GHKM and Tree
  --ternary             extract ternary rule

Max memory allocation:
  --max-malloc=MALLOC   maximum memory in GB (default: 8)

Directories:
  --toolkit-dir=DIRECTORY
                        toolkit directory
  --mpi-dir=DIRECTORY   MPI directory

MPI options: (if one of mpi/mpi-host/mpi-host-file is set, we will run by MPI)
  --mpi=MPI             # of processes for MPI-based parallel processing.
                        Identical to --np for mpirun
  --mpi-host=HOSTS      list of hosts to run job. Identical to --host for
                        mpirun
  --mpi-host-file=FILE  host list file to run job. Identical to --hostfile for
                        mpirun
Threading option: (for non-MPI applications)
  --threads=THREADS     # of thrads for thread-based parallel processing

Running on cluster using PBS Pro batch job scheduler:
  --pbs                 PBS for launching processes
  --pbs-queue=NAME      PBS queue for launching processes (default: ltg)

Others:
  --debug=DEBUG         
  -h, --help            show this help message and exit

