Phrase/Rule/Tree-fragment extraction

We support various phrase/rule/tree extraction. Basically, we will run in two steps:
   Collect counts by mapping input sentences/parse trees/alignment etc.
   Map/Reduce to collect target-side counts
   Map/Reduce to collect source/target counts and produce various counts

   They can operate similarly with underlying extraction specific map/reduce framework.

You can try "cicada-extract.py" which runs like phrase-extract in Moses, but starting from step 4 (and 6).
It is recommended to specify TMPDIR_SPEC (or TMPDIR) pointing to 
a large "local" storage for efficient extraction. In NICT, you can exploit local disc on running machines
(and cluster machines):

export TMPDIR_SPEC=/export/tmp

Internally, cicada-extract.py calls following binaries:

cicada_extract_phrase{,_mpi}
	Extract phrase
	Output scores:
	       lhs ||| rhs ||| count(lhs, rhs) \
	       	   count(prev, mono, lhs, rhs) \
		   count(prev, swap, lhs, rhs) \
		   count(next, mono, lhs, rhs) \
		   count(next, swap, lhs, rhs)

cicada_extract_rule{,_mpi}
	Extract synchronous-CFG + syntax augmentation (aka SAMT) when extracted with "span" data.
	You can generate span by "cicada_filter_penntreebank"
	Output scores:
	       root lhs ||| root rhs ||| count(lhs, rhs)

cicada_extract_ghkm{,_mpi}
	Extract GHKM rules, tree-to-string or string-to-tree rules
	Output scores:
	       lhs-xRS ||| rhs-xRS ||| count(lhs, rhs)
	TODO: How to support dumping in string-to-tree?
	      add --inverse option for inversed alignment,
	      add --swap option for swapping source and target?

cicada_extract_tree{,_mpi}
	Extract GHKM rules, tree-to-tree rules
	Output scores:
	       lhs-xRS ||| rhs-xRS ||| count(lhs, rhs)

After counts collection, you can summarize them by cicada_extract_counts{,_mpi}
Which will output:

lhs ||| rhs ||| counts(lhs, rhs) ||| counts(lhs) ||| counts(rhs) ||| observed(lhs) observed(rhs) \
    	||| lex(lhs | rhs) lex(rhs | lhs)

In addtion, we will dump root-source.gz and root-target.gz. root-source.gz contains:

root(lhs) ||| counts(root(lhs)) ||| observed(lhs,rhs) observed(lhs)

while root-target.gz looks like:

root(rhs) ||| counts(root(rhs)) ||| observed(hs,rhs) observed(rhs)

You can easily transform the counts into probabilities by maximum likelihood estimates, or use observed counts
to perform Dirichlet prior smoothing (default).
Following filers are implemented:

   cicada_filter_extract:
	extract only nbet of target variation for each source side,
   	measured by its joint frequency of lhs and rhs.

   cicada_filter_extract_phrase:
	Dump in moses or cicada format. Also, you can dump
	lexicalied reordering table.
   				 
   cicada_filter_extract_scfg:
	Dump in cicada format for synchronous-CFG. You can also 
	add features for lhs given root(lhs) and rhs given root(rhs)

   cicada_filter_extract_ghkm:
	Dump in cicada format for tree-to-string, string-to-tree, tree-to-tree.

Remarks on SCFG: (I don't know the default for Joshua... anybody has any clue?)
	If you want to simulate the Moses-style hierarchical rule, you can use "cicada-extract.py" with options:
	
	--max-span-source 15
	--max-span-target 15
	--min-hole-source 2
	--min-hole-target 1
	--max-length 5
	--max-fertility 0
	--exhaustive

	The Hiero-style extraction from Chiang (2007) will be:

	--max-span-source 15
	--max-span-target 0
	--min-hole-source 2
	--min-hole-target 1
	--max-length 5(?)
	--max-fertility 0

	Our default:

	--max-span-source 15
	--max-span-target 20
	--min-hole-source 1
	--min-hole-target 1
	--max-length 7
	--max-fertility 4
