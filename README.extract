Phrase/Rule/Tree-fragment extraction

We (will) support various phrase/rule/tree extraction. Basically, we will run in two steps:
   Collect counts by mapping input sentences/parse trees/alignmeent etc.
   Map/Reduce to collect target-side counts
   Map/Reduce to collect source/target counts and produce various counts

   They can operate similarly with underlying extraction specific map/reduce framework.

cicada_extract_phrase{,_mpi}
	Extract phrase
	Output scores:
	       lhs ||| rhs ||| count(lhs, rhs) count(prev, mono, lhs, rhs) count(prev, swap, lhs, rhs) count(next, mono, lhs, rhs) count(next, swap, lhs, rhs)

cicada_extract_rule{,_mpi}
	Extract synchronous-CFG + support syntax augmentation.
	Output scores:
	       root lhs ||| root rhs ||| count(lhs, rhs)

cicada_extract_ghkm{,_mpi}
	Extract GHKM rules, tree-to-string or string-to-tree rules
	Output scores:
	       lhs-xRS ||| rhs-xRS ||| count(lhs, rhs)

After counts collection, you can summarize them by cicada_extract_counts{,_mpi}
Which will output:

lhs ||| rhs ||| counts(lhs, rhs) ||| counts(lhs) ||| counts(rhs) ||| observed(lhs) observed(rhs) ||| lex(lhs | rhs) lex(rhs | lhs)

In addtion, we will dump rule-source.gz and rule-target.gz which looks like

root(lhs) ||| counts(root(lhs)) ||| observed(lhs,rhs) observed(lhs)

for rule-source.gz and rule-target.gz looks like:

root(rhs) ||| counts(root(rhs)) ||| observed(hs,rhs) observed(rhs)

You can easily transform the counts into probabilities by maximum likelihood estimates, or use observed counts
to perform Dirichlet prior smoothing.

