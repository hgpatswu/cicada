Do we allow state-less features applied during composition???
	We need composition-based features for phrase-based MT
	to compute distortion/lexicalized reordering etc. not represented in
	hypergraph struct

	State-less features are sometimes related to composition algorithm,
	such as shift-reduce etc.
	
	We have "parse" variant of "compose". Do we allow stateless features during "parsing" ????

Better MIRA training
    Optimized online training by combining MERT procedure to select better update ratio.... (does it work?)
    Sharding to reduce memory for each rank
    Separate averaging and non-averaging code during optimization...

Systematic symbols:
  - So forth, we have abused many symbols, such as |:;+ for binarization, permutation, parent, etc.
    I think it's time to systemacically define symbols so that we can de-binarize, de-parentize etc.

Add head-finder?
    Head finder requires child/parent node, therefore, requires the whole hypergraph...

Re-engineer kbest/viterbi framework...
  I don't like current interface, separating weight and derivation
  Is there a workaround, like use iterator-based enumeration?

Better training via MERT like procedure:
  We already have LBFGS(L1,L2) and SGD(L1, L2).
  Add MIRA etc. originally implemented for learn_online...

Support word alignment training!
  We know feature-rich unsupervised training!
  Use of syntax-tree combined with alignment
  cube-pruning to transform source-syntactic tree into alignment-derivation! (use of "alignment" as our output...)
  Can we integrate supervised learning and unsupervised learning sharing the same code...?
      We can potentially share by modifying "intersection"
  How to encode lexical feature...?
    use grammar! (source target word mapping with features!)
  Intersection with the supplied alignment: supervised training!
    Can we do this, given that word alignment space is rather constrained...

Reimplement cicada-learn-online:
  Distribute sentences and oracle translations first, then, run online learning
  instead of distributing sentences on the fly... This will potentially speed up
  learning at the cost of slow decoding at some sentences... We can potentially avoid this by 
  sorting training data wrt its length.
  
  or, implement as cicada_learn_online_static{,_mpi} ?? since we will pre-demine spliting...

Cutting-plane learning under cicada_maxlike{,_mpi}...
	We need to merge feature vector...

Implement scalable training by L1 regularized feature selection:
  1. Collect features.. it is the hardest part...
     Decode-and-extract features
     or use simple featueres (word-pairs etc) from bitext?
  2. Compute importance
  3. Learn w/o introducing "new features": modify not to introduce new features

  Remarks: what kind of features?
     features easily collectable from bilingual data (with monolingual data), (pairs, ngrams etc.)
     or, 
     features requing complex hidden variables...?

POS annotated sentence input:
  How to convert each POS into latent annotated POS?
  Solution 1. Implemenet GrammarUnknownPOS which will give annotated POS when already in the
  model. Otherwise use POS as a "hint"?

Add more tests

Add mert,decode,mbr,parse script?
    Personally, I don't like a single script to handle many things, since 
    that is usually not instructive....

A unified framework to collect statistics: Do we need stats from features?
    We have a support to collect stats from operations, but not from individual features..

Cube summing?
     I'm still not sure how to efficiently collect residuals... Do we include state-less features
     to serve as residual counts?
  
Implement RIBES for mteval
   Can we implement RIBES feature...?
   It is global in that we have to perform "alignment" as in METEOR...

Use parse-cky inside parse-coarse instead of compose-cky for (potentially) faster parsing with less memory consumption

Add incremental version of MERT?

reimplement liblinear functionality...???
   liblinear does not allow starting from an intermediata state, like alpha from previous iterations
   liblinear does not run in threading

Add MPI version of Model1/HMM alignment?

Model1 starting from dice-estimated parameters...??

Integrate cicada-alignment.py and cicada-extract.py???
   Currently, NO, in order to encourange user to select an alternative paths.

Integrate MPI and non-MPI of cicada_learn{,kbest}{,mpi}
   Use the same gradient,margin computers across mpi and non-mpi applications
   