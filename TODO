VERSIONING!

Do we allow state-less features applied during composition???
	We need composition-based features for phrase-based MT
	to compute distortion/lexicalized reordering etc. not represented in
	hypergraph struct

	State-less features are sometimes related to composition algorithm,
	such as shift-reduce etc.

Support tree-to-tree transduction:
	Support non-tree transducer matching:
	match with phrases or hierarchical phrases whose spans match with
	syntactic spans.
	experimental code exits, but which do not support features/attributes in original hypergraph....

Better MIRA training
    Optimized online training by combining MERT procedure to select better update ratio.... (does it work?)

Use source side tree-fragment as a rule, and parse "lattice" (or, string-to-tree, as in ISI system!)
    Simply convert source side into a flat structure and parse via CKY
    	   + additional checking for target-side!
    Allow special encoding in tree-grammar...

Add matcher interface?
  - for lattice generation, it is reported stemming or word net synset does help to generalize matching.
    Do we implement this as our string baseline?

Systematic symbols:
  - So forth, we have abuced many symbols, such as |:;+ for binarization, permutation, parent, etc.
    I think it's time to systemacically define symbols so that we can de-binarize, de-parentize etc.

Add head-finder?
    Head finder requires child/parent node, therefore, requires the whole hypergraph...

Add alignment tool, which can read GIZA++ output and dump heuristic alignment?

Add pharse-extract like script supporting phrase-extract only (step 5 and 6)

Do we allow intersect with lattice, not the first sentence of targets...?

Re-engineer kbest/viterbi framework...
  I don't like current interface, separating weight and derivation
  Is there a workaround, like use iterator-based enumeration?

Better training via MERT like procedure:
  We already have LBFGS(L1,L2) and SGD(L1, L2).
  Add MIRA etc. originally implemented for learn_online...

Support word alignment training!
  We know feature-rich unsupervised training!
  Use of syntax-tree combined with alignment
  cube-pruning to transform source-syntactic tree into alignment-derivation! (use of "alignment" as our output...)
  Can we integrate supervised learning and unsupervised learning sharing the same code...?
      We can potentially share by modifying "intersection"
  How to encode lexical feature...?
    use grammar! (source target word mapping with features!)
