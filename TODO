Do we allow state-less features applied during composition???
	We need composition-based features for phrase-based MT
	to compute distortion/lexicalized reordering etc. not represented in
	hypergraph struct

	State-less features are sometimes related to composition algorithm,
	such as shift-reduce etc.
	
	We have "parse" variant of "compose". Do we allow stateless features during "parsing" ????

Systematic symbols:
  - So forth, we have abused many symbols, such as |:;+ for binarization, permutation, parent, etc.
    I think it's time to systemacically define symbols so that we can de-binarize, de-parentize etc.

Add head-finder?
    Head finder requires child/parent node, therefore, requires the whole hypergraph...

Re-engineer kbest/viterbi framework...
  I don't like current interface, separating weight and derivation
  Is there a workaround, like use iterator-based enumeration?

Support word alignment training!
  We know feature-rich unsupervised training!
  Use of syntax-tree combined with alignment
  cube-pruning to transform source-syntactic tree into alignment-derivation! (use of "alignment" as our output...)
  Can we integrate supervised learning and unsupervised learning sharing the same code...?
      We can potentially share by modifying "intersection"
  How to encode lexical feature...?
    use grammar! (source target word mapping with features!)
  Intersection with the supplied alignment: supervised training!
    Can we do this, given that word alignment space is rather constrained...


Implement scalable training by L1 regularized feature selection:
  1. Collect features.. it is the hardest part...
     Decode-and-extract features
     or use simple featueres (word-pairs etc) from bitext?
  2. Compute importance
  3. Learn w/o introducing "new features": modify not to introduce new features

  Remarks: what kind of features?
     features easily collectable from bilingual data (with monolingual data), (pairs, ngrams etc.)
     or, 
     features requing complex hidden variables...?

POS annotated sentence input:
  How to convert each POS into latent annotated POS?
  Solution 1. Implemenet GrammarUnknownPOS which will give annotated POS when already in the
  model. Otherwise use POS as a "hint"?

Add more tests

A unified framework to collect statistics: Do we need stats from features?
    We have a support to collect stats from operations, but not from individual features..

Cube summing?
     I'm still not sure how to efficiently collect residuals... Do we include state-less features
     to serve as residual counts?
  
Implement RIBES for mteval
   Can we implement RIBES feature...?
   It is global in that we have to perform "alignment" as in METEOR...

Use parse-cky inside parse-coarse instead of compose-cky for (potentially) faster parsing with less memory consumption

Add incremental version of MERT?

reimplement liblinear functionality...???
   liblinear does not allow starting from an intermediata state, like alpha from previous iterations
   liblinear does not run in threading
   
   Implement with template techniques...?

Correctly implement cutting-plane algorithms:
   we need to merge feature vector into a bundle, then, iterative add new vectors...
   do we implement in maxlike families...?

Add MPI version of Model1/HMM alignment?

Model1 starting from dice-estimated parameters...??

Dictionary constrained Model1/HMM training?
   As in GIZA++, we will constraint by the existence of dictionary items

Alignment constrained alignment
   As observed in training, we will constraint the alignment space in training.
   Do we also constrain for the viterbi/ITG/MaxMatch alignment?

Integrate cicada-alignment.py and cicada-extract.py???
   Currently, NO, in order to encourange user to select an alternative paths.

Integrate MPI and non-MPI of cicada_learn{,kbest}{,mpi}
   Use the same gradient,margin computers across mpi and non-mpi applications

Global lexicon learning by liblinear...?
   We will add bias feature, then learn...

Threaded cicada:
  It turned out to be difficult: we can clone models and grammars, but 
  the problem is when... Actually, we should clone "after" threads are created,
  assuming that the models/grammars are "untouched" by the main thread.

Collapse and binarize the source side of the ghkm/tree grammar
   Perform "collapsing" and do binarization
   Perform lexicalization?
   Perform synchronous binarization? (we need to store all in memory...?)
     and left-to-right and target-left-to-right binarization
   How to represent intermedidate non-terminals?
     We need to keep track of both-side....???
     
Do we keep pair of source-target lhs???
  I'm pretty not sure how symbols should be handled during the CKY algorithm.
  Probably, we need to keep both side and maintain it during unary rules...?

Remove sub-tree sharing in compose-tree-cky and parse-tree-cky?
  I'm not sure which is better..

Support joshua training?
Support cicada forest converter?

Deprecate old learn-online code...?
  since it is very complicated and error-prone, almost impossible to maintain.
  or, simply deprecate non-mpi version?

hypergraph variant of learn-block?

Add softmax-margin... How? (assume duplicates as in mert code?)

octavian support?