VERSIONING!

Do we allow state-less features applied during composition???
	We need composition-based features for phrase-based MT
	to compute distortion/lexicalized reordering etc. not represented in
	hypergraph struct

	State-less features are sometimes related to composition algorithm,
	such as shift-reduce etc.

Better MIRA training
    Optimized online training by combining MERT procedure to select better update ratio.... (does it work?)
    Sharding to reduce memory for each rank
    Separate averaging and non-averaging code during optimization...

Use source side tree-fragment as a rule, and parse "lattice" (or, string-to-tree, as in ISI system!)
    Simply convert source side into a flat structure and parse via CKY
    	   + additional checking for target-side!
    Allow special encoding in tree-grammar...
    
Re-engineer tree-grammar/tree-transducer code:
   Use special edge_tye to encapsulate edge-id so that we can differentiate
   from a conventional "flat" tree-fragment, used in string-to-tree decoding.

Systematic symbols:
  - So forth, we have abuced many symbols, such as |:;+ for binarization, permutation, parent, etc.
    I think it's time to systemacically define symbols so that we can de-binarize, de-parentize etc.

Add head-finder?
    Head finder requires child/parent node, therefore, requires the whole hypergraph...

Re-engineer kbest/viterbi framework...
  I don't like current interface, separating weight and derivation
  Is there a workaround, like use iterator-based enumeration?

Better training via MERT like procedure:
  We already have LBFGS(L1,L2) and SGD(L1, L2).
  Add MIRA etc. originally implemented for learn_online...

Support word alignment training!
  We know feature-rich unsupervised training!
  Use of syntax-tree combined with alignment
  cube-pruning to transform source-syntactic tree into alignment-derivation! (use of "alignment" as our output...)
  Can we integrate supervised learning and unsupervised learning sharing the same code...?
      We can potentially share by modifying "intersection"
  How to encode lexical feature...?
    use grammar! (source target word mapping with features!)

Reimplement cicada-learn-online:
  Distribute sentences and oracle translations first, then, run online learning
  instead of distributing sentences on the fly... This will potentially speed up
  learning at the cost of slow decoding at some sentences... We can potentially avoid this by 
  sorting training data wrt its length.
  
  or, implement as cicada_learn_online_static{,_mpi} ?? since we will pre-demine spliting...

Added binarization to cicada-extract, or use cicada tool for binarization?

Revise parent feature:
  we do not trust the label from rule, but use state propagated root label...
